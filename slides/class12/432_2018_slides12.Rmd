---
title: "432 Class 12 Slides"
author: "github.com/THOMASELOVE/432-2018"
date: "2018-02-22"
output:
  beamer_presentation:
    theme: "Madrid"
    colortheme: "lily"
    fonttheme: "structurebold"
    fig_caption: FALSE
---

```{r set-options, echo=FALSE, cache=FALSE}
knitr::opts_chunk$set(comment=NA)
options(width = 60)
```

## Setup

```{r, warning = FALSE, message = FALSE}
library(skimr)
library(pROC)
library(ROCR)
library(rms) # note: also loads Hmisc
library(simputation)
library(broom)
library(tidyverse)
```

## Today's Materials

- Logistic Regression and the Framingham Study (conclusion)
- Performing Linear Regression with `ols`
- Hormone Therapy and Baseline LDL in the `HERS` trial 

The HERS trial is described in Vittinghoff et al., especially Chapter 4.

# Logistic Regression and Framingham

## Data Ingest, Cleanup (from Class 10)

```{r}
fram <- read.csv("data/fram_new.csv") %>% tbl_df
set.seed(432001)
fram1 <- fram %>%
    impute_pmm(educ + cigs_day + heart_r ~ age + smoker) %>%
    impute_rlm(bmi + tot_chol ~ sex + age + sbp + heart_r) %>%
    impute_pmm(bp_meds ~ hx_htn + bmi + tot_chol) %>%
    impute_rlm(glucose ~ hx_dm + bmi + tot_chol + age) %>%
    mutate(ed_f = fct_recode(factor(educ),
                   "1_Some_HS" = "1", "2_HS_grad" = "2",
                   "3_Some_Col" = "3", "4_Col_grad" = "4"))
fram2 <- fram1 %>%
    select(subj, sex, age, smoker, cigs_day, bp_meds,
           hx_stroke, hx_htn, hx_dm, ed_f, tot_chol,
           sbp, dbp, bmi, heart_r, glucose, CHD_10)
```

## The Models We've Fit (predicting `CHD_10`)

```{r}
m_01 <- glm(CHD_10 ~ hx_htn, data = fram2, 
            family = binomial)

d <- datadist(fram2)
options(datadist = "d")
m_01_lrm <- lrm(CHD_10 ~ hx_htn, data = fram2, x = T, y = T)

m_02 <- glm(CHD_10 ~ hx_htn + tot_chol, 
            data = fram2, family = binomial)
m_02_lrm <- lrm(CHD_10 ~ hx_htn + tot_chol, data = fram2,
                x = TRUE, y = TRUE)
```

## Then We Fit a Kitchen Sink Model (Class 11)

```{r}
m_03 <- glm(CHD_10 ~ hx_htn + tot_chol + sex + age +
                    smoker + cigs_day + bp_meds + 
                    hx_stroke + hx_dm + ed_f + sbp + dbp +
                    bmi + heart_r + glucose, 
                data = fram2, family = binomial)

d <- datadist(fram2)
options(datadist = "d")
m_03_lrm <- lrm(CHD_10 ~ hx_htn + tot_chol + sex + age +
                    smoker + cigs_day + bp_meds + 
                    hx_stroke + hx_dm + ed_f + sbp + dbp +
                    bmi + heart_r + glucose, 
                data = fram2, x = TRUE, y = TRUE)
```

## Comparing the Current Models

Model | C statistic | Nagelkerke R^2^ 
----------: | ------: | -------: 
`m_01_lrm` | 0.614 | 0.051 
`m_02_lrm` | 0.640 | 0.055 
`m_03_lrm` | 0.733 | 0.159 

`m_03_lrm` shows the following predictors as highly significant:

- `age`, `sex`, `sbp`, `cigs_day`, `glucose`, and `hx_stroke`

# Fitting a 6-predictor, but still useful model

## What looks useful?

By ANOVA on `m_03_lrm` it looks like `age`, `sex`, `sbp`, `cigs_day`, `glucose`, `hx_stroke` for sure.

```{r, fig.height = 3}
plot(spearman2(CHD_10 ~ age + sex + sbp + cigs_day + 
                   glucose + hx_stroke, data = fram2))
```

## New Model 4

```{r}
m_04 <- glm(CHD_10 ~ rcs(age, 5) + rcs(sbp, 3) + sex + 
                    hx_stroke + glucose + cigs_day, 
                data = fram2, family = binomial)


dd <- datadist(fram2)
options(datadist = "dd")

m_04_lrm <- lrm(CHD_10 ~ rcs(age, 5) + rcs(sbp, 3) + sex + 
                    hx_stroke + glucose + cigs_day,
                data = fram2, x = TRUE, y = TRUE)
```

## `m_04_lrm`

![](figures/figF_09.png)

## Validating our Summary Statistics

```{r}
set.seed(432329); validate(m_04_lrm)[1:4,]
```

## `plot(summary(m_04_lrm))`

```{r, echo = FALSE}
plot(summary(m_04_lrm))
```

## `plot(anova(m_04_lrm))`

```{r, echo = FALSE}
plot(anova(m_04_lrm))
```

## Can we see the prediction results?

```{r}
ggplot(Predict(m_04_lrm), 
       anova = anova(m_04_lrm), pval = TRUE)
```

## What about on a better scale?

```{r}
ggplot(Predict(m_04_lrm, fun = plogis))
```

## Calibration of `mod_04_lrm`

```{r}
set.seed(432612); plot(calibrate(m_04_lrm))
```

## Goodness of fit test?

```{r}
round(residuals(m_04_lrm, type = "gof"),3)
```

## Nomogram of `mod_04_lrm`

```{r}
plot(nomogram(m_04_lrm, fun = plogis))
```

## Comparing Models 3 and 4 (which aren't nested)

```{r}
glance(m_03) # kitchen sink but no non-linear terms
glance(m_04) # six predictors but with non-linear terms
```

## Checking Residuals?

- Yes/No outcomes contain less information than quantitative outcomes
- Residuals cannot be observed - predicted
    + There are several different types of residuals defined
- Assumptions of logistic regression are different
    + Model is deliberately non-linear
    + Error variance is a function of the mean, so it isn't constant
    + Errors aren't assumed to follow a Normal distribution
    + Only thing that's the same: leverage and influence

So, plot 5 (residuals/leverage/influence) can be a little useful, but that's it.

- We'll need better diagnostic tools for generalized linear models.

## Any observations particularly influential on Model 4?

```{r}
which.influence(m_04_lrm, cutoff = 0.3)
```

## Influence and Model 4?

```{r}
plot(m_04, which = 5)
```

# The HERS Trial

## Hormone Therapy and Baseline LDL in the HERS Trial

HERS clinical trial of hormone therapy (HT). We're excluding the women with diabetes here.

```{r}
hers <- read.csv("data/hersdata.csv") %>% tbl_df

hers1 <- hers %>%
    filter(diabetes == "no") %>%
    select(subject, LDL, HT, age, smoking, drinkany, SBP,
           physact, BMI, diabetes)
```

## The Data

```{r}
head(hers1)
```

## The Codebook (n = 2,032 women without diabetes)

Variable   | Description | Missing?
---------: | --------------------------------- | ---
`subject`  | subject code | 0
`LDL`      | LDL cholesterol in mg/dl | 7
`HT`       | factor: hormone therapy or placebo | 0
`age`      | age in years | 0 
`smoking`  | yes or no | 0
`drinkany` | yes or no | 2
`SBP`      | systolic BP in mm Hg | 0
`physact`  | 5-level factor | 0
`BMI`      | body-mass index in kg/m^2^ | 2
`diabetes` | yes or no (all of these are no) | 0

## Our Modeling Goal

Predict `LDL` using 

- `age`
- `smoking`
- `drinkany`
- `SBP`
- `physact`
- `BMI`
- the interaction of `smoking` and `BMI`

## Details on `physact` variable

```{r}
hers1 %>% count(physact)
```

## Skim?

```{r, eval = FALSE}
hers1 %>% select(-subject) %>% skim()
```

![](figures/fig01.png)

## Missingness pattern?

```{r}
na.pattern(hers1) # from Hmisc
names(hers1)
```

### Next slide

```{r, eval = FALSE}
par(mfrow = c(2,2))
naplot(naclus(hers1))
par(mfrow = c(1,1))
```

## `naplot(naclus(hers1))`

```{r, echo = FALSE}
par(mfrow = c(2,2))
naplot(naclus(hers1))
par(mfrow = c(1,1))
```


# Simple Imputation into `hers2`

## Simple Imputation for `drinkany`, `BMI` and `LDL`

Since `drinkany` is a factor, we have to do some extra work to impute.

```{r}
set.seed(432092)

hers2 <- hers1 %>%
    mutate(drinkany_n = 
               ifelse(drinkany == "yes", 1, 0)) %>%
    impute_pmm(drinkany_n ~ age + smoking) %>%
    mutate(drinkany = 
               ifelse(drinkany_n == 1, "yes", "no")) %>%
    impute_rlm(BMI ~ age + smoking + SBP) %>%
    impute_rlm(LDL ~ age + smoking + SBP + BMI) 
```

## Now, check missingness...

```{r}
na.pattern(hers2) 
names(hers2)
```

# Multiple Imputation with `aregImpute`

## Multiple Imputation using `aregImpute` from `Hmisc`

Model to predict all missing values of any variables, using additive regression bootstrapping and predictive mean matching.

Steps are:

1. `aregImpute` draws a sample with replacement from the observations where the target variable is observed, not missing. 
2. It then fits a flexible additive model to predict this target variable while finding the optimum transformation of it. 
3. It then uses this fitted flexible model to predict the target variable in all of the original observations.
4. Finally, it imputes each missing value of the target variable with the observed value whose predicted transformed value is closest to the predicted transformed value of the missing value.

## Fitting a Multiple Imputation Model

```{r}
set.seed(4320132)
dd <- datadist(hers1)
options(datadist = "dd")
fit3 <- aregImpute(~ LDL + age + smoking + drinkany +
                       SBP + physact + BMI, 
                   nk = c(0, 3:5), tlinear = FALSE,
                   data = hers1, B = 10, n.impute = 20) 
```


## Multiple Imputation using `aregImpute` from `Hmisc`

`aregImpute` requires specifications of all variables, and several other details:

- `n.impute` = number of imputations, we'll run 20
- `nk` = number of knots to describe level of complexity, with our choice `nk = c(0, 3:5)` we'll fit both linear models and models with restricted cubic splines with 3, 4, and 5 knots
- `tlinear = FALSE` allows the target variable to have a non-linear transformation when `nk` is 3 or more
- `B = 10` specifies 10 bootstrap samples will bs used
- `data` specifies the source of the variables


## `aregImpute` Imputation Results (1 of 3)

```{r, eval = FALSE}
fit3
```

![](figures/fig02.png)

## `aregImpute` Imputation Results (2 of 3)

![](figures/fig03.png)

## `aregImpute` Imputation Results (3 of 3)

![](figures/fig04.png)

## A plot of the imputed values... (results)

```{r, echo = FALSE}
par(mfrow = c(1,3))
plot(fit3)
par(mfrow = c(1,1))
```


## A plot of the imputed values... (code)

```{r, eval = FALSE}
par(mfrow = c(1,3))
plot(fit3)
par(mfrow = c(1,1))
```

- For `LDL`, we imputed most of the 7 missing subjects in most of the 20 imputation runs to values within a range of around 120 through 200, but occasionally, we imputed values that were substantially lower than 100. 
- For `drinkany` we imputed about 70% no and 30% yes.
- For `BMI`, we imputed values ranging from about 23 to 27 in many cases, and up near 40 in other cases. 
- This method never imputes a value for a variable that doesn't already exist in the data.

## Spearman $\rho^2$ Plot

We've already decided to include a `BMI`*`smoking` product term, but how should we prioritize the degrees of freedom we spend on non-linearity otherwise?

```{r, eval = F}
plot(spearman2(LDL ~ age + smoking + drinkany + SBP + 
                   physact + BMI, data = hers2))
```

## Spearman $\rho^2$ Plot Result

```{r, echo = F}
plot(spearman2(LDL ~ age + smoking + drinkany + SBP + 
                   physact + BMI, data = hers2))
```

# Fitting a Linear Regression with `ols`

## Model we'll fit

Fitting a model to predict `LDL` using

- `BMI` with a restricted cubic spline, 5 knots
- `age` with a quadratic polynomial
- `SBP` as a linear term
- `drinkany` indicator
- `physact` factor
- `smoking` indicator and its interaction with `BMI`

We could fit this to the data

- restricted to complete cases (hers1, effectively)
- after simple imputation (hers2)
- after our multiple imputation (fit3)

## Fitting the model after simple imputation

```{r}
dd <- datadist(hers2)
options(datadist = "dd")

m2 <- ols(LDL ~ rcs(BMI, 5) + pol(age, 2) + SBP + 
              drinkany + physact + smoking + 
              smoking %ia% BMI, data = hers2,
          x = TRUE, y = TRUE)
```

where `%ia%` identifies the linear interaction alone.

## `m2` results (slide 1 of 2)

![](figures/fig05.png)

## `m2` results (slide 2 of 2)

![](figures/fig06.png)

## Validation of summary statistics

```{r}
validate(m2)
```

## `anova(m2)` results

![](figures/fig07.png)

## `summary(m2)` results

![](figures/fig08.png)

## `plot(summary(m2))` results

```{r, echo = FALSE}
plot(summary(m2))
```

## `plot(nomogram(m2))`

```{r, echo = FALSE}
plot(nomogram(m2))
```

## Making Predictions for an Individual

Suppose now that we want to use R to get a prediction for a new individual subject with `BMI` = 30, `age` = 50, `smoking` = yes and `physact` = about as active, `drinkany`= yes and `SBP` of 150.

```{r, eval = FALSE}
predict(m2, expand.grid(BMI = 30, age = 50, smoking = "yes",
                        physact = "about as active", 
                        drinkany = "yes", SBP = 150),
        conf.int = 0.95, conf.type = "individual")
```

```
$linear.predictors        $lower     $upper
          160.9399      88.48615   233.3936
```

## Making Predictions for a Long-Run Mean

The other kind of prediction we might wish to make is for the mean of a series of subjects whose `BMI` = 30, `age` = 50, `smoking` = yes and `physact` = about as active, `drinkany`= yes and `SBP` of 150.

```{r, eval = FALSE}
predict(m2, expand.grid(BMI = 30, age = 50, smoking = "yes",
                        physact = "about as active", 
                        drinkany = "yes", SBP = 150),
        conf.int = 0.95, conf.type = "mean")
```

```
$linear.predictors        $lower     $upper
          160.9399      151.8119   170.0679
```

Of course, the confidence interval will always be narrower than the prediction interval given the same predictor values.

## Influential Points?

```{r}
which.influence(m2, cutoff = 0.4)
```

## Fitting the model to the complete cases

```{r}
d <- datadist(hers1)
options(datadist = "d")

m1 <- ols(LDL ~ rcs(BMI, 5) + pol(age, 2) + SBP + 
              drinkany + physact + smoking + 
              smoking %ia% BMI, data = hers1,
          x = TRUE, y = TRUE)
```

where `%ia%` identifies the linear interaction alone.

# Putting it Together

## What have we got?

- An imputation model `fit3`

```
fit3 <- aregImpute(~ LDL + age + smoking + drinkany + SBP + 
           physact + BMI, nk = c(0, 3:5), tlinear = FALSE,
           data = hers1, B = 10, n.impute = 20, x = TRUE)
```

- A prediction model

```
m1 <- ols(LDL ~ rcs(BMI, 5) + pol(age, 2) + SBP +
            drinkany + physact + smoking + smoking %ia% BMI,
            x = TRUE, y = TRUE)
```

Now we put them together

## Linear Regression & Imputation Model

```{r}
m3imp <- 
    fit.mult.impute(LDL ~ rcs(BMI, 5) + pol(age, 2) + SBP +
                        drinkany + physact + smoking + 
                        smoking %ia% BMI,
                    fitter = ols, xtrans = fit3, 
                    data = hers1)
```

## `m3imp` results (1 of 2)

![](figures/fig09.png)

## `m3imp` results (2 of 2)

![](figures/fig10.png)

## `anova(m3imp)`

![](figures/fig11.png)

## Evaluation via Partial R^2^ and AIC (result)

```{r, echo = FALSE}
par(mfrow = c(1,2))
plot(anova(m3imp), what="partial R2")
plot(anova(m3imp), what="aic")
par(mfrow = c(1,1))
```

## Evaluation via Partial R^2^ and AIC (code)

```{r, eval = FALSE}
par(mfrow = c(1,2))
plot(anova(m3imp), what="partial R2")
plot(anova(m3imp), what="aic")
par(mfrow = c(1,1))
```

## `summary(m3imp)`

![](figures/fig12.png)

## `plot(summary(m3imp))`

```{r, echo = FALSE}
plot(summary(m3imp))
```

## `plot(resid(m3imp) ~ fitted(m3imp))`

```{r, echo = FALSE}
plot(resid(m3imp) ~ fitted(m3imp))
```

## `plot(nomogram(m3imp))`

```{r, echo = FALSE}
plot(nomogram(m3imp))
```
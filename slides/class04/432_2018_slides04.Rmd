---
title: "432 Class 4 Slides"
author: "github.com/THOMASELOVE/432-2018"
date: "2018-01-25"
output:
  beamer_presentation:
    theme: "Madrid"
    colortheme: "lily"
    fonttheme: "structurebold"
    fig_caption: FALSE
---

```{r set-options, echo=FALSE, cache=FALSE}
knitr::opts_chunk$set(comment=NA)
options(width = 60)
```

## Setup

```{r, warning = FALSE, message = FALSE}
library(skimr)
library(simputation)
library(broom)
library(modelr)
library(tidyverse)

smartcle1 <- read.csv("data/smartcle1.csv") 
```

## Today's Materials

- Prediction and Confidence Intervals
- Centering and Rescaling Predictors
- Two-Factor Analysis of Variance
- More to come...

## Last time, we built `smartcle3` and two models...

```{r create_smartcle3}
set.seed(20180123)

smartcle3 <- smartcle1 %>%
  select(SEQNO, bmi, sleephrs, female, alcdays, exerany) %>%
  impute_rhd(exerany ~ 1) %>%
  impute_pmm(sleephrs ~ 1) %>%
  impute_rlm(bmi ~ female + sleephrs) %>%
  impute_cart(alcdays ~ .) %>%
  tbl_df()

model_int <- lm(bmi ~ female * sleephrs, data = smartcle3)
model_noint <- lm(bmi ~ female + sleephrs, data = smartcle3)
```

## Building Predictions for New Data (Individual Subjects)

What do we predict for the `bmi` of a female subject who gets 10 hours of sleep per night? What if the subject was male, instead?

```{r}
new1 <- data_frame(female = c(1, 0), sleephrs = c(10,10))

predict(model_int, newdata = new1, 
        interval = "prediction", level = 0.95)
```

## Building Predictions for New Data (Average Predictions)

What do we predict for the average `bmi` of a population of female subjects who sleep for 10 hours? What about the population of male subjects?

```{r}
new1 <- data_frame(female = c(1, 0), sleephrs = c(10,10))

predict(model_int, newdata = new1, 
        interval = "confidence", level = 0.95)
```

# Centering and Rescaling Predictors (See Notes sections 2.13, 2.14 and 4.7)

## Centering `sleephrs` to ease interaction description

```{r}
smartcle3 <- smartcle3 %>% 
  mutate(sleep_c = sleephrs - mean(sleephrs))

model_int_c <- lm(bmi ~ female * sleep_c, data = smartcle3)
model_int_c
```

## Interpreting Interaction: Centered `sleephrs`

`bmi` = 28.23 - 0.68 `female` + 0.04 centered `sleep_c` - 0.45 `female` x centered `sleep_c`

- Now, 28.23 is the predicted `bmi` for a male who gets the average amount of sleep (7.02 hours)
- And 28.23 - 0.68 = 27.55 is the predicted `bmi` for a female who gets the average amount of sleep.
- So, the main effect of `female` is the predictive difference (female - male) in `bmi` for mean `sleephrs`,
- the product term is the change in the slope of centered `sleephrs_c` on `bmi` for a female rather than a male, and
- the residual standard deviation and the R-squared values remain unchanged from the model before centering.

```{r}
glance(model_int_c) %>% round(., 3)
```

## Plotting `bmi` on centered `sleep_c` by `female`

```{r, echo = FALSE}
ggplot(smartcle3, aes(x = sleep_c, y = bmi, group = female, 
                      col = factor(female))) +
    geom_point(alpha = 0.5, size = 2) +
    geom_smooth(method = "lm", se = FALSE) +
    guides(color = FALSE) +
    labs(x = "Sleep Hours, centered", y = "Body Mass Index",
         title = "Model bmi using sleep_c and female") +
    facet_wrap(~ female, labeller = label_both)
```

## Rescaling?

Centering helped us interpret the main effects in the regression, but it still leaves a scaling problem.

- The female coefficient estimate is much larger than that of sleephrs, but this is misleading, considering that we are comparing the complete change in one variable (sex = female or not) to a 1-hour change in average sleep.
- Gelman and Hill (2007) recommend all continuous predictors be scaled by dividing by 2 standard deviations
    - A 1-unit change in the rescaled predictor corresponds to a change from 1 standard deviation below the mean, to 1 standard deviation above.
    - An unscaled binary (1/0) predictor with 50% probability of occurring will be exactly comparable

## Rescaling to `sleep_z` and re-fitting the model

```{r}
smartcle3 <- smartcle3 %>%
    mutate(sleep_z = (sleephrs - mean(sleephrs)) /
             (2*sd(sleephrs)))

model_int_z <- lm(bmi ~ female * sleep_z, data = smartcle3)

model_int_z
```

## Comparing our Interaction Models

Original Model

- `bmi` = 27.95 + 2.47 `female` + 0.04 `sleephrs` - 0.45 `female` x `sleephrs`

Centered Model

- `bmi` = 28.23 - 0.68 `female` + 0.04 `sleep_c` - 0.45 `female` x `sleep_c`

Centered, Rescaled Model

- `bmi` = 28.23 - 0.68 `female` + 0.12 `sleep_z` - 1.37 `female` x `sleep_z`

## Interpreting the Centered, Rescaled Model

- Main effect of `female`, -0.68, is still the predictive difference (female - male) in `bmi` with `sleephrs` at its mean, 7.02 hours,
- Intercept (28.23) is still the predicted `bmi` for a male who sleeps the mean number of hours, and
- the residual standard deviation and the R-squared values remain unchanged

but now we also have:

- the coefficient of `sleep_z` is the predictive difference in bmi associated with a change in `sleephrs` of 2 standard deviations (from one standard deviation below the mean of 7.02 to one standard deviation above 7.02.)
    - Since sd(sleephrs) is 1.52, this corresponds to a change from 5.50 hours per night to 8.54 hours per night.
- the coefficient of the product term (-1.37) corresponds to the change in the coefficient of `sleep_z` for females as compared to males.

## Plotting the Rescaled, Centered Model

```{r, echo = FALSE}
ggplot(smartcle3, aes(x = sleep_z, y = bmi, 
                  group = female, col = factor(female))) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", size = 1.5) +
    scale_color_discrete(name = "Is subject female?") +
    labs(x = "Sleep Hours, centered and standardized (2 sd)",
         y = "Body Mass Index",
         title = "Interaction model: centered, rescaled sleephrs")
```

# Two-Factor Analysis of Variance (see Notes Chapter 3)

## How do `female` and `exerany` relate to `bmi`?

```{r}
smart3_sum <- smartcle3 %>%
  group_by(female, exerany) %>%
  summarize(mean.bmi = mean(bmi), sd.bmi = sd(bmi))
```

## Resulting tibble for `smart3_sum`

```{r}
smart3_sum
```

This would be more useful as a plot.

## Building a Means Plot (result on next slide)

```{r, eval = FALSE}
pd <- position_dodge(0.2)

ggplot(smart3_sum, aes(x = exerany, y = mean.bmi, 
                       col = factor(female))) +
  geom_errorbar(aes(ymin = mean.bmi - sd.bmi,
                    ymax = mean.bmi + sd.bmi),
                width = 0.2, position = pd) +
  geom_point(size = 2, position = pd) + 
  geom_line(aes(group = female), position = pd) +
  scale_color_discrete(name = "Female?") +
  theme_bw() +
  labs(y = "Body Mass Index", 
       x = "Exercise at all in past 30 days?",
       title = "Means (+/- SD) of BMI by Exercise and Sex")
```

## Means Plot (Do we have a strong interaction effect?)

```{r, echo = FALSE}
pd <- position_dodge(0.2)

ggplot(smart3_sum, aes(x = exerany, y = mean.bmi, 
                       col = factor(female))) +
  geom_errorbar(aes(ymin = mean.bmi - sd.bmi,
                    ymax = mean.bmi + sd.bmi),
                width = 0.2, position = pd) +
  geom_point(size = 2, position = pd) + 
  geom_line(aes(group = female), position = pd) +
  scale_color_discrete(name = "Female?") +
  theme_bw() +
  labs(y = "Body Mass Index", 
       x = "Exercise at all in past 30 days?",
       title = "Means (+/- SD) of BMI by Exercise and Sex")
```

## Two-Way ANOVA model with Interaction

```{r}
model2 <- lm(bmi ~ female * exerany, data = smartcle3)

anova(model2)
```

Does it seem like we need the interaction term in this case?

## Summary of Two-Factor ANOVA with Interaction

![](figs/fig02.png)

## What if we wanted the model with no interaction?

Here's the key plot, then...

```{r, eval = FALSE}
p1 <- ggplot(smartcle3, aes(x = factor(female), y = bmi)) + 
    geom_boxplot()
p2 <- ggplot(smartcle3, aes(x = factor(exerany), y = bmi)) +
    geom_boxplot()

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

## Key Plot for Two-Way ANOVA, no interaction

```{r, echo = FALSE}
p1 <- ggplot(smartcle3, aes(x = factor(female), y = bmi)) + 
    geom_boxplot()
p2 <- ggplot(smartcle3, aes(x = factor(exerany), y = bmi)) +
    geom_boxplot()

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

## Two-Way ANOVA model without Interaction

```{r}
model2_noint <- lm(bmi ~ female + exerany, data = smartcle3)

anova(model2_noint)
```

## Summary of Two-Factor No Interaction ANOVA

![](figs/fig03.png)

## Tukey HSD Comparisons (no interaction)

```{r, echo = FALSE}
par(mfrow=c(1,2))
plot(TukeyHSD(aov(bmi ~ factor(female) + factor(exerany), 
                  data = smartcle3)))
par(mfrow=c(1,1))
```

## Tukey HSD Comparisons (without interaction)

```{r, echo = FALSE}
TukeyHSD(aov(bmi ~ factor(female) + factor(exerany), 
             data = smartcle3))
```

## Tukey HSD comparisons WITH interaction

```{r, echo = FALSE}
par(mfrow=c(1,3))
plot(TukeyHSD(aov(bmi ~ factor(female) * factor(exerany), 
                  data = smartcle3)))
par(mfrow=c(1,1))
```

## Tukey HSD comparisons WITH interaction

![](figs/fig04.png)

## Indicator Variables

What if I used (1 = yes, 2 = no) instead of (1 = yes, 0 = no) for `exerany`? What if I tell R that `exerany` is a factor?

```{r}
smartcle3 <- smartcle3 %>%
  mutate(exer_12 = 2 - exerany,
    exer_yn = fct_recode(factor(exerany), Y = "1", N = "0"))

smartcle3 %>% count(exerany, exer_12, exer_yn)
```

## Two-Predictor model with `exerany` (1 = yes, 0 = no)

```{r}
lm(bmi ~ exerany * alcdays, data = smartcle3)
```

## Two-Predictor model with `exer_12` (1 = yes, 2 = no)

```{r}
lm(bmi ~ exer_12 * alcdays, data = smartcle3)
```

Compare to 

` (Intercept)    exerany    alcdays  exerany:alcdays`  
`   29.79211    -2.10499   -0.10141          0.02546`  

## Two-Predictor model with `exer_yn` (factor)

```{r}
lm(bmi ~ exer_yn * alcdays, data = smartcle3)
```

Compare to 

` (Intercept)    exerany    alcdays  exerany:alcdays`  
`   29.79211    -2.10499   -0.10141          0.02546`  

# Fitting Linear Regressions, and then Validating Them

## A Linear Regression for `bmi` from `smartcle3`

```{r}
mod_ks <- lm(bmi ~ female + sleephrs + alcdays + exerany, 
             data = smartcle3)
round(coef(mod_ks),2)
glance(mod_ks)
```

## `tidy(mod_ks)`

```{r, echo = FALSE}
tidy(mod_ks)
```

## ANOVA for sequential testing of predictors

```{r}
anova(mod_ks)
```

## Different order but the same model?

```{r}
anova(lm(bmi ~ exerany + alcdays + female + sleephrs, 
         data = smartcle3))
```

## Does Order Matter? Comparing Slopes 

`model_ks` Order | Estimate 
-------------: | ---------: 
Intercept | 32.32 
`female` | -1.19 
`sleephrs` | -0.24 
`alcdays` |  -0.10 
`exerany` |  -2.15 

Revised Order | Estimate 
-------------: | ---------: 
Intercept | 32.32 
`exerany` |  -2.15 
`alcdays` |  -0.10 
`female` |  -1.19 
`sleephrs` |  -0.24 

## Does Order Matter? Comparing *t* test and CI results

- t tests in `summary` and `tidy` test value as "last predictor in"

`model_ks` Order | Estimate | t test *p*  | 95% CI
-------------: | ---------: | ---------: | -------------
Intercept | 32.32 | < 2e-16  | (30.3, 34.3)
`female` | -1.19 |    0.0028 | (-2.0, -0.4)
`sleephrs` | -0.24 |  0.0499 | (-0.5, -0.0)
`alcdays` |  -0.10 | 7.9e-05 | (-0.14, -0.05)
`exerany` |  -2.15 | 1.8e-06 | (-3.0, -1.2)

Revised Order | Estimate | t test *p*  | 95% CI
-------------: | ---------: | ---------: | -------------
Intercept | 32.32 | < 2e-16 |  (30.3, 34.3)
`exerany` |  -2.15 | 1.8e-06 | (-2.0, -0.4)
`alcdays` |  -0.10 | 7.9e-05 | (-0.5, -0.0)
`female` |  -1.19 |  0.0028 | (-0.14, -0.05)
`sleephrs` |  -0.24 | 0.0499 | (-3.0, -1.2)

## Does Order Matter? Comparing Slopes and *p* values

- t tests in `summary` and `tidy` test value as "last predictor in"
- anova tests of a single `lm` consider predictive value "in sequence"

`model_ks` Order | Estimate | t test *p*  | ANOVA *p*
-------------: | ---------: | ---------: | ---------: 
Intercept | 32.32 | < 2e-16 | -
`female` | -1.19 |    0.0028 | 0.075
`sleephrs` | -0.24 |  0.0499 | 0.073
`alcdays` |  -0.10 | 7.9e-05 | 2.1e-05
`exerany` |  -2.15 | 1.8e-06 | 1.8e-06

Revised Order | Estimate | t test *p*  | ANOVA *p*
-------------: | ---------: | ---------: | ---------: 
Intercept | 32.32 | < 2e-16 | -
`exerany` |  -2.15 | 1.8e-06 | 1.7e-06
`alcdays` |  -0.10 | 7.9e-05 | 0.0007
`female` |  -1.19 |  0.0028 | 0.0025
`sleephrs` |  -0.24 | 0.0499 | 0.0499

# Do we need all of those variables in `mod_ks`? (Sections 7-8)

## Stepwise Regression (backwards elimination)

```{r}
step(mod_ks)
```

## Stepwise Regression (forwards selection)

```{r, eval = FALSE}
with(smartcle3,
     step(lm(bmi ~ 1),
          scope = (~ exerany + alcdays + female + sleephrs),
          direction = "forward"))
```

## Forward Selection Stepwise Regression, Results: 1

![](figs/fig05.png)

## Forward Selection Stepwise Regression, Results: 2

![](figs/fig06.png)

## Forward Selection Stepwise Regression, Results: 3

![](figs/fig07.png)

## Conclusions?

- Forward selection and backwards elimination show the same model, which is also the kitchen sink model.
    - Does that mean that the model is right?
    - Does that mean that the model is good?
    - Does that mean that the model is the best possible combination of these predictors?

>- Should we feel substantially more confident about the above statements when the forward selection result = the backwards elimination result, as in our model for `bmi` using `smartcle3`?

>- No.

# Validating the Model (See Section 6)

## Training and Test Samples (as in 431)

Suppose we want to evaluate whether our `model_ks` predicts effectively in new data. 

One approach (used, for instance, in 431) would be to split our sample into a separate training (perhaps 70% of the data) and test (perhaps 30% of the data) samples, and then:

1. fit the model in the training sample,
2. use the resulting model to make predictions for `bmi` in the test sample, and
3. evaluate the quality of those predictions, perhaps by comparing the results to what we'd get using a different model.

But there are problems with this approach, especially if *n* is small.

## What else could we do?

Suppose we're afraid that our model building and testing will be hampered by a small sample size. 

- A potential solution is the idea of **cross-validation**, which involves partitioning our data into a series of training-test subsets, multiple times, and then combining the results. 
- So, in the next slides, I'll show you how to do something called **10-fold cross validation** using some tools from the `modelr` package, which is a non-core part of the `tidyverse`.

## 10-fold cross validation: The idea

1. Split the 1,036 observations in our `smartcle3` data frame into a partition of about 90% (so about 932 observations) for a training sample, leaving the remaining 10% (about 104 observations) for a test sample. Label the test sample `.id` = 1 in R.
2. Refit our model (here, kitchen sink) to the training sample, and use it to predict our outcome (`bmi`) in the test sample.
3. Store the prediction results for the subjects in the test sample.
4. Split the observations again, ensuring that a completely new 10% gets held out for the test sample, labeling this new test sample `.id` = 2 in R. Then redo parts 2 and 3. Now you have prediction results for 20% of the subjects in the original data.
5. Repeat the process (10x in total) until you have prediction results for all 100% of the subjects in the original data. Thus, each observation is used 9 times in the training sample, and once in the test sample.

## 10-fold cross-validation of `mod_ks`

```{r}
set.seed(432021)

sink_models <- smartcle3 %>%
    crossv_kfold(k = 10) %>%
    mutate(model = map(train, ~ 
                         lm(bmi ~ female + sleephrs + 
                              alcdays + exerany, data = .)))

sink_predictions <- sink_models %>%
    unnest(map2(model, test, ~ augment(.x, newdata = .y)))
```

## The first few cross-validated predictions

```{r}
head(sink_predictions, 3)
```

## Graphing the Cross-Validated Prediction Errors of `bmi` (code)

```{r, eval = FALSE}
sink_predictions %>%
    mutate(errors = bmi - .fitted) %>%
    ggplot(., aes(x = errors)) +
    geom_histogram(bins = 30, fill = "darkviolet", 
                   col = "yellow") + 
    labs(title = "Cross-Validated Errors Predicting BMI",
         subtitle = "Kitchen Sink model, smartcle3",
         x = "Error in predicting BMI")
```

## Cross-Validated Prediction Errors of `bmi`

```{r, echo = FALSE}
sink_predictions %>%
    mutate(errors = bmi - .fitted) %>%
    ggplot(., aes(x = errors)) +
    geom_histogram(bins = 30, fill = "darkviolet", 
                   col = "yellow") + 
    labs(title = "Cross-Validated Errors Predicting BMI",
         subtitle = "Kitchen Sink model, smartcle3",
         x = "Error in predicting BMI")
```

## Summary Statistics Based on Cross-Validated Prediction Errors

We'll look at the **root mean squared prediction error** or RMSE, and the **mean absolute error**, too.

```{r}
sink_predictions %>%
    summarize(RMSE_sink = sqrt(mean((bmi - .fitted) ^2)),
              MAE_sink = mean(abs(bmi - .fitted)))
```

## Comparison to a Model with the Intercept Only (predict mean BMI)?

```{r}
sink_predictions %>%
    summarize(RMSE_sink = sqrt(mean((bmi - .fitted) ^2)),
        RMSE_intercept = sqrt(mean((bmi - mean(bmi))^2)),
        MAE_sink = mean(abs(bmi - .fitted)),
        MAE_intercept = mean(abs(bmi - mean(bmi)))) %>%
  round(., 3)
```

## Next Week

- Homework 1 discussion in class Tuesday
- Stepwise Regression via the Allen-Cady Procedure
- Best Subsets approaches to Variable Selection
- Making Decisions about Non-Linearity in Y or in the Xs


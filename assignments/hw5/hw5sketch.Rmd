---
title: "Assignment 5 Answer Sketch"
author: "432 Staff"
date: "Due 2018-03-02. Sketch developed `r Sys.Date()`"
output:
  html_document:
    code_folding: show
    toc: TRUE
    toc_float: TRUE
    number_sections: TRUE
  pdf_document:
    toc: TRUE
    number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment=NA)
```

```{r package_load, message = FALSE}
library(skimr)
library(simputation)
library(Epi)
library(broom)
library(rms)
library(tidyverse)

skim_with(numeric = list(hist = NULL), 
          integer = list(hist = NULL)) # drop histograms
```

# Question 1 (25 points)

We don't write answer sketches for essay questions.

## Instructions for Data for Questions 2-9

> The data come from the [NHANES National Youth Fitness Survey](http://wwwn.cdc.gov/nchs/nhanes/search/nnyfs12.aspx). Data collected in the `nnyf1.csv` file above and on the [Data and Code] page from our site come from the **Demographics** files, and from the **Medical Conditions** and **Physical Activity** files, which are each part of the **Questionnaire** data.

> I merged files on the basis of the respondent sequence number (SEQN). The variables available to you are:

1. `SEQN` - the *respondent sequence number* (there are 1,576 subject in the `nnyfs1.csv` file made available to you)
2. `RIASEX` (from the Demographics files) - *sex of subject (1 = male, 2 = female)*
3. `RIDAGEYR` (from the Demographics files) - *age in years at screening (3-15)*
4. `RIDRETH1` (from the Demographics files) - *race/hispanic origin (1 = Mexican-American, 2 = Other Hispanic, 3 = Non-Hispanic White, 4 = Non-Hispanic Black, 5 = Other Race including Multi-Racial)*
5. `INDFMPIR` (from the Demographics files; **impute** all subjects with missing values on the basis of `RIDRETH1` and `RIDAGEYR`) - *ratio of family income to poverty (data show 0-4.99, and then truncated as 5 for all who are in fact greater than or equal to 5)*
6. `MCQ010` (from the Medical Conditions files; all subjects should have values of 1 [Yes] or 2 [No]) - *has the child ever been told they have asthma*
7. `PAQ706` (from the Physical Activity files; **drop** all subjects with values other than 0, 1, 2, 3, 4, 5, 6, or 7) - *days (in the past 7) physically active at least 60 minutes*

## Data Management for Questions 2-9

**NOTE** I'm going to make a meal out of this here, getting every little scrap of adjustment to the data I will use in any of the questions that follow done here. You probably did some management of data as you went through, and probably were smart enough to drop some of the details (like sanity checks, and rechecking with tables, counts, summaries, skims, etc.) That's totally fine with me. So long as you explain what you actually did to modify the data, that's the main thing.


First, let's look at the data immediately after import, to see if any of the values we observe don't match what we've been led to believe from the description above. Specifically, we want to verify that we have 1,576 observations at the start.

```{r}
nnyfs1.raw <- read.csv("nnyfs1.csv") %>% tbl_df

nnyfs1.raw
```

and we do have 1,576 observations.

We have also been told that:

1. `RIASEX` and `MCQ010` has values of 1 or 2 and nothing else. Is that true?

```{r}
nnyfs1.raw %>% count(RIASEX, MCQ010)
```

Yes, it seems correct, We see only values of 1 and 2 for each variable.

2. `RIDRETH1` is supposed to have only integer values between 1 and 5. Is that true?

```{r}
nnyfs1.raw %>% count(RIDRETH1)
```

Again, looks good.

3. `RIDAGEYR` has values between 3 and 15. Is that the case? Since the data are actually stored in integer years, we can still use `count` to check this, or we could use a simple summary or skim to look at the maximum and minimum values. Let's do both here, to show you what I mean.

```{r}
nnyfs1.raw %>% count(RIDAGEYR)
```

```{r}
nnyfs1.raw %>% skim
```

We can see from either approach that the minimum `RIDAGEYR` is in fact 3 and that the maximum is 15.

4. `INDFMPIR` should have values between 0 and 5 (probably with many values of 5)

We can see from the `skim` and its presentation of the minimum (0) and maximum (5) that we're probably all right, though we don't yet know how many 5s we actually have. Let's find out.

```{r}
nnyfs1.raw %>% count(INDFMPIR == 5)
```

We have 196 values of 5, and 100 missing values, with the remaining values falling between 0 and 4.99. So we'll need to impute soon.

5. `PAQ706` has a series of values, but we're going to drop anything that isn't an integer between 0 and 7

The minimum (from the skim) is 0 and the maximum is 99. From the skim, we see these are integers, so we can count them.

```{r}
nnyfs1.raw %>% count(PAQ706)
```

It turns out we have 3 missing and 3 more 99 values that we'll need to drop. The remaining observation should stay.

So, our cleanup tasks are:

1. Drop the values of `PAQ706` above 7, including the three missing and three 99 cases.
2. Impute the remaining missing `INDFMPIR` values (100 now, may be less after we drop the problematic `PAQ706` cases.)

Once that is done, we can answer question 2, but we'll do some additional cleanup anticipating what we'll need in Questions 3-9.

### Drop the values of `PAQ706` outside of the 0-7 range

There are six subjects who need to be removed here. We can do that by retaining only those subjects with `PAQ706` < 8.

```{r}
nnyfs1.new <- nnyfs1.raw %>%
    dplyr::filter(PAQ706 < 8)

nnyfs1.new %>% count(PAQ706)
```

### Dealing with missing values in `INDFMPIR`

Since all observed `INDFMPIR` values are in 0-5 (note the minimum and maximum above), we need only impute the missing values. I'll use simple imputation, and predictive mean matching using all of the other variables in the data.

```{r}
set.seed(4321243)

nnyfs1.new <- nnyfs1.new %>%
    impute_pmm(INDFMPIR ~ RIASEX + RIDAGEYR + 
                   RIDRETH1 + MCQ010 + PAQ706) 

# check to ensure that all missing values are successfully imputed
nnyfs1.new %>% select(INDFMPIR) %>% skim
```

### Re-specifying and re-naming variables

I'd like these variables to be more useful to me in modeling work. So, I will:

1. Create 1/0 well-named representations (called `female` and `asthma`, respectively) of the two binary variables, `RIASEX` and `MCQ010`, to make Questions 4-9 easier. 
2. Create well-named descriptive factor representations (called `sex` and `asthma_f`) of the two binary variables, `RIASEX` and `MCQ010`, to make Question 3 easier. I'll also move the FEMALES to the front of the list of levels for `sex`.
3. Create a new 1/0 representation of whether `RIDRETH1` is 3 to help in Question 9.

```{r}
nnyfs1.new <- nnyfs1.new %>%
    mutate(female = ifelse(RIASEX == 2, 1, 0),
           asthma = ifelse(MCQ010 == 1, 1, 0),
           sex = fct_recode(factor(RIASEX), 
                            M = "1", F = "2"),
           sex = fct_relevel(sex, "F"),
           asthma_f = fct_recode(factor(MCQ010), 
                                Yes = "1", No = "2"),
           nonh_white = ifelse(RIDRETH1 == 3, 1, 0))
```

### Some Sanity Checks

Do the results in our three different versions of the response about asthma match up?

```{r}
nnyfs1.new %>% count(MCQ010, asthma, asthma_f)
```

Yes.

Do the results in our three different versions of the response about sex match up?

```{r}
nnyfs1.new %>% count(sex, RIASEX, female)
```

Yes.

Do the results in our `nonh_white` match what we were trying to get out of `RIDRETH1`?

```{r}
nnyfs1.new %>% count(RIDRETH1, nonh_white)
```

Yes, that's right.

### One Last skim to see what I've done

```{r}
nnyfs1.new %>% skim
```

Looks good.

- The `asthma`, `female` and `nonh_white` binary variables are numeric, as planned, and take the values 0 and 1.
- The `asthma_f` and `sex` variables are factors, as expected.
- and we have no missingness remaining. I think we're all set.



# Question 2 (5 points)

> How many of those subjects wind up in your final data set, after applying the inclusion and exclusion criteria described above?

```{r}
nrow(nnyfs1.new)
```

# Question 3 (10 points)

> Find the cross-product odds ratio and an appropriate 95% confidence interval for that odds ratio for being told you have asthma for females as compared to males within this sample. Specify the relevant cross-tabulation (contingency table).

```{r}
twoby2(nnyfs1.new$sex, nnyfs1.new$asthma_f)
```

The cross-product odds ratio is 1.29, with 95% CI (0.99, 1.68). The odds of a female being told they have asthma are estimated to be 1.29 times that of a male in this sample, but the confidence interval still includes 1, so the effect size doesn't meet the 5% significance level standard. 

*Note* that this question was substantially easier to do with the respecified and (in the case of `sex` also reordered) factor variables to display the information about sex and asthma.

# Question 4 (5 points)

> Use a logistic regression model to predict: `MCQ010` "Ever been told you have asthma" = YES [1] on the basis of the following variables: on the basis of the following variables: sex (captured in an indicator of `female`), subject's age at screening, Ratio of family income to poverty, and number of days physically active in the past 7. Specify the equation of the model you have fit.

Note that we expected you to treat the number of days physically active as a quantitative predictor, rather than as a factor. Some of you may have instead treated it as a factor, which is a really bad idea (using a factor in this case for this variable ignores the fact that it is a count, adds a lot of complexity and chews up a lot of degrees of freedom, for no meaningful gain. And it makes question 7 enormously more complicated than it needs to be.) I didn't absolutely prevent you from treating it as a factor, but doing so makes your life much harder, though. In this sketch, I'll do the simpler and sensible thing.

**Note**: Your answers will differ from ours because of the imputation of `INDFMPIR`. This will affect questions 4-9, a little bit.

## Approach 1: Using `glm` to fit the model

`PAQ706` is treated here as quantitative...

```{r}
(m4 <- glm(asthma ~ female + RIDAGEYR + INDFMPIR + PAQ706, 
          data = nnyfs1.new, family="binomial"))
```

Model `m4` reads as follows:

- The log odds of asthma = -1.66 + 0.22 `female` + 0.07 `RIDAGEYR` - 0.12 `INDFMPIR` - 0.08 `PAQ706`

if you used our random seed to help with the imputation. Your answer will be a little different, perhaps.

## Approach 2: Using `lrm` to fit the model

```{r}
d <- datadist(nnyfs1.new)
options(datadist="d")

(m4_lrm <- lrm(asthma ~ female + RIDAGEYR + INDFMPIR + 
                   PAQ706, data = nnyfs1.new, x=T, y=T))
```

This is, of course, the same model we displayed before as `m4`.

# Question 5 (10 points)

> Specify and interpret the model's odds ratio estimate for being told you have asthma for females as compared to males, after adjusting for the other variables included in the model you fit in Question 4. Provide a 95% confidence interval for this odds ratio.

We can use either `glm` or `lrm` to accomplish this, since the predictor we're studying is binary.

With `glm`, we'd use...

```{r}
exp(coef(m4))
exp(confint(m4, level = 0.95))
```

And the odds ratio estimate is 1.247, with 95% CI (0.95, 1.63).

With `lrm`, we'd use...

```{r}
summary(m4_lrm)
```

And again, get an odds ratio for `female`'s effect of 1.247, with 95% CI (0.95, 1.63).

Note that for the non-binary variables, the effect estimates look different, as they should.

- The `glm` approach presents confidence intervals for the effect of increasing a predictor by 1, holding everything else constant.
- The `lrm` approach presents confidence intervals for the effect of increasing a predictor from what is listed in the summary output as Low to High (usually the 25th to 75th percentiles) for quantitative predictors.

- Either way, the model estimates that a female will have 1.24 [95% CI 0.96-1.63] times the odds of being told they have asthma compared to a male with the same values of the age, ratio of family income to poverty, and days physically active in the past 7. Again, the difference attributable to sex is not enough for us to call it significant at the 5% level.

# Question 6 (10 points)

> Compare your result in Question 3 to your result in Question 5. Are they different? If so, why?

Question | Odds Ratio Estimate | 95% Confidence Interval
-------: | :----------: | :--------------:
3 (unadj.) | 1.29 | (0.99, 1.68)
5 (adj.) | 1.24 | (0.95, 1.63)

The results are slightly different, because Question 5's model adjusts for age at screening, ratio of family income to poverty and days physically active in the past, but those adjustments don't make an enormous practical difference in the estimate. In either case, we would (barely) fail to reject a null hypothesis of no `sex` effect, at the 5% significance level.

# Question 7 (10 points)

> Specify and interpret the Question 4 model's odds ratio estimate (and a 95% confidence interval around that point estimate) associated with the "days physically active in the past 7" predictor. 

We have to choose what we're showing here, based on the modeling strategy, as we noted in the answer to Question 5.

- If we show the `glm` result, we will be estimating the odds ratio associated with a change of 1 day of exercise. That turns out to be 0.92, with 95% CI (0.87, 0.98).
- If we show the `lrm` result, we will be estimating the odds ratio associated with a change of 3 days in exercise, from 4 to 7, specifically, which is a much larger change. So it's not surprising that odds ratio is further from 1. Specifically it is 0.78, with 95% CI (0.65, 0.95).

# Question 8 (10 points)

> Use the model you fit in Question 4 to provide a prediction for the probability that a 10-year-old male child will have been told they have asthma, if they are active 3 days in the past 7, and have a ratio of family income to poverty of 2.5.

To answer this question, we'll create a little data frame (called `question8`) containing the data for this new subject.

```{r}
question8 <- data_frame(female = 0, RIDAGEYR = 10, PAQ706 = 3, INDFMPIR = 2.5)
```

With the `glm` approach, our predicted probability is easily obtained:

```{r}
predict(m4, newdata = question8, type = "response")
```

With the `lrm` approach, we can run ...

```{r}
predict(m4_lrm, newdata = question8, type="fitted.ind")
```

The predicted probability is 0.18

# Question 9 (15 points)

> Refit the model you fit in Question 4 but now, add in an additional predictor variable that indicates if the subject's race/Hispanic origin value is Non-Hispanic White (i.e. RIDRETH1 = 3), or not. Decide whether or not an interaction term between age and race/ethnicity is required (but do not consider other interaction terms or other types of non-linearity). Specify the logistic regression equation you wind up fitting. 

## Using `glm`

We'll fit the models with and without the interaction term, and then assess them.

```{r}
(m9_int <- glm(asthma ~ female + INDFMPIR + PAQ706 + 
                  nonh_white*RIDAGEYR, 
              data = nnyfs1.new, family="binomial"))

(m9_noint <- glm(asthma ~ female + INDFMPIR + PAQ706 + 
                  nonh_white + RIDAGEYR, 
                data = nnyfs1.new, family="binomial"))
```

We could compare the models using:

- Analysis of Deviance
- AIC
- BIC

```{r}
anova(m9_int)
```

```{r}
anova(m9_noint, m9_int)
```

Either way, the drop in deviance is 0.021, using up 1 df, which is nowhere near statistically significant. The *p* value is about 0.89 for the interaction term.

```{r}
pchisq(0.02075, 1, lower.tail = FALSE)
```

```{r}
glance(m9_noint)
glance(m9_int)
```

The AIC and BIC are lower for the model without the interaction, as well.

So, using any of those methods, the model wiout the interaction seems more appropriate. That model is:

- log odds of `asthma` = -1.66 + 0.22 `female` - 0.13 `INDFMPIR` - 0.08 `PAQ706` + 0.07 `nonh_white` + 0.07 `RIDAGEYR`

## Using the `lrm` approach

We can again fit the two models, and then compare them with ANOVA, AIC or BIC, and probably also several other measures.

```{r}
(m9_int_lrm <- lrm(asthma ~ female + INDFMPIR + PAQ706 + 
                  nonh_white*RIDAGEYR, 
                  data = nnyfs1.new, x=TRUE, y=TRUE))

(m9_noint_lrm <- lrm(asthma ~ female + INDFMPIR + PAQ706 + 
                  nonh_white + RIDAGEYR, 
                  data = nnyfs1.new, x=TRUE, y=TRUE))
```

For instance, the models with and without interaction have the same C statistic, and Nagelkerke R^2^, so the interaction cannot be doing much, and the *p* value for the `nonh_white * RIDAGEYR` interaction term is, again, 0.89. I could run `AIC` and `BIC` again for the `lrm` fit, but they will yield the same results we saw previously.

Any way you look at it, the interaction term doesn't add anything of importance to the model, so a model without interaction seems more sensible. That model is:

- log odds of `asthma` = -1.66 + 0.22 `female` - 0.13 `INDFMPIR` - 0.08 `PAQ706` + 0.07 `nonh_white` + 0.07 `RIDAGEYR`

